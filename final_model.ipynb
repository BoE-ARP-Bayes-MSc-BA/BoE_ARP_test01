{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# read the csv file\n",
    "df = pd.read_csv('df_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>meeting_text</th>\n",
       "      <th>file_name</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>749</td>\n",
       "      <td>Ladies and gentlemen. Good afternoon. Welcome,...</td>\n",
       "      <td>20190424_AXA_SA-_Shareholder_Mtg_Call_2019-4-2...</td>\n",
       "      <td>2019-04-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1492</td>\n",
       "      <td>Greetings. Welcome to the Markel Corporation F...</td>\n",
       "      <td>20120510_Markel_Corp-_Earnings_Call_2012-5-10_...</td>\n",
       "      <td>2012-05-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>446</td>\n",
       "      <td>Good morning or good afternoon. Welcome to Swi...</td>\n",
       "      <td>20190731_Swiss_Re_AG-_Earnings_Call_2019-7-31_...</td>\n",
       "      <td>2019-07-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1438</td>\n",
       "      <td>Market Cap: 69,035.17. Current PX: 148.33. YTD...</td>\n",
       "      <td>20170726_Chubb_Ltd-_Earnings_Call_2017-7-26_FS...</td>\n",
       "      <td>2017-07-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>337</td>\n",
       "      <td>Yasuyoshi Karasawa. Good afternoon, ladies and...</td>\n",
       "      <td>20190524_MS-AD_INSURANCE-_Guidance_Call_2019-5...</td>\n",
       "      <td>2019-05-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>423</td>\n",
       "      <td>Ladies and gentlemen, good morning or good aft...</td>\n",
       "      <td>20160429_Swiss_Re_AG-_Earnings_Call_2016-4-29_...</td>\n",
       "      <td>2016-04-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>96</td>\n",
       "      <td>Welcome, everyone, to our presentation today. ...</td>\n",
       "      <td>20200526_Aviva_PLC-_Shareholder_Mtg_Call_2020-...</td>\n",
       "      <td>2020-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>40</td>\n",
       "      <td>Operator. (Operator Instructions) Good day, la...</td>\n",
       "      <td>20191204_Sanlam_Ltd-_Sales_Results_Call_2019-1...</td>\n",
       "      <td>2019-12-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916</th>\n",
       "      <td>605</td>\n",
       "      <td>Greetings, and welcome to the Lancashire Holdi...</td>\n",
       "      <td>20150729_LANCASHIRE_HOLDI-_Earnings_Call_2015-...</td>\n",
       "      <td>2015-07-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917</th>\n",
       "      <td>1115</td>\n",
       "      <td>Operator. Good day, and welcome to the Gjensid...</td>\n",
       "      <td>20181025_GJENSIDIGE_FORSI-_Earnings_Call_2018-...</td>\n",
       "      <td>2018-10-25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>918 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                       meeting_text  \\\n",
       "0           749  Ladies and gentlemen. Good afternoon. Welcome,...   \n",
       "1          1492  Greetings. Welcome to the Markel Corporation F...   \n",
       "2           446  Good morning or good afternoon. Welcome to Swi...   \n",
       "3          1438  Market Cap: 69,035.17. Current PX: 148.33. YTD...   \n",
       "4           337  Yasuyoshi Karasawa. Good afternoon, ladies and...   \n",
       "..          ...                                                ...   \n",
       "913         423  Ladies and gentlemen, good morning or good aft...   \n",
       "914          96  Welcome, everyone, to our presentation today. ...   \n",
       "915          40  Operator. (Operator Instructions) Good day, la...   \n",
       "916         605  Greetings, and welcome to the Lancashire Holdi...   \n",
       "917        1115  Operator. Good day, and welcome to the Gjensid...   \n",
       "\n",
       "                                             file_name        date  \n",
       "0    20190424_AXA_SA-_Shareholder_Mtg_Call_2019-4-2...  2019-04-24  \n",
       "1    20120510_Markel_Corp-_Earnings_Call_2012-5-10_...  2012-05-10  \n",
       "2    20190731_Swiss_Re_AG-_Earnings_Call_2019-7-31_...  2019-07-31  \n",
       "3    20170726_Chubb_Ltd-_Earnings_Call_2017-7-26_FS...  2017-07-26  \n",
       "4    20190524_MS-AD_INSURANCE-_Guidance_Call_2019-5...  2019-05-24  \n",
       "..                                                 ...         ...  \n",
       "913  20160429_Swiss_Re_AG-_Earnings_Call_2016-4-29_...  2016-04-29  \n",
       "914  20200526_Aviva_PLC-_Shareholder_Mtg_Call_2020-...  2020-05-26  \n",
       "915  20191204_Sanlam_Ltd-_Sales_Results_Call_2019-1...  2019-12-04  \n",
       "916  20150729_LANCASHIRE_HOLDI-_Earnings_Call_2015-...  2015-07-29  \n",
       "917  20181025_GJENSIDIGE_FORSI-_Earnings_Call_2018-...  2018-10-25  \n",
       "\n",
       "[918 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# import torch\n",
    "# from tqdm import tqdm\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from torch.utils.data import DataLoader,Dataset # 可以繼承 類似map and unmap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from transformers import BertTokenizer,BertModel,get_linear_schedule_with_warmup \n",
    "import seaborn as sns\n",
    "# import nlpaug.augmenter.sentence as nas\n",
    "# import nlpaug.augmenter.char as nac\n",
    "# import nlpaug.augmenter.word as naw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "  pattern = re.compile(r'\\s+')\n",
    "  text=text.replace('\\u202f',' ').replace('xa0',' ')\n",
    "  text=re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", text)\n",
    "  text=re.sub(r\"\\\\u00b4\",r'',text)\n",
    "  text=re.sub(pattern, ' ', text)\n",
    "  text = text.replace('?', ' ? ').replace(')', ') ').strip()\n",
    "  return text\n",
    "\n",
    "df['meeting_text']=df['meeting_text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>meeting_text</th>\n",
       "      <th>file_name</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>749</td>\n",
       "      <td>Ladies and gentlemen Good afternoon Welcome de...</td>\n",
       "      <td>20190424_AXA_SA-_Shareholder_Mtg_Call_2019-4-2...</td>\n",
       "      <td>2019-04-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1492</td>\n",
       "      <td>Greetings Welcome to the Markel Corporation Fi...</td>\n",
       "      <td>20120510_Markel_Corp-_Earnings_Call_2012-5-10_...</td>\n",
       "      <td>2012-05-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>446</td>\n",
       "      <td>Good morning or good afternoon Welcome to Swis...</td>\n",
       "      <td>20190731_Swiss_Re_AG-_Earnings_Call_2019-7-31_...</td>\n",
       "      <td>2019-07-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1438</td>\n",
       "      <td>Market Cap 6903517 Current PX 14833 YTD Change...</td>\n",
       "      <td>20170726_Chubb_Ltd-_Earnings_Call_2017-7-26_FS...</td>\n",
       "      <td>2017-07-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>337</td>\n",
       "      <td>Yasuyoshi Karasawa Good afternoon ladies and g...</td>\n",
       "      <td>20190524_MS-AD_INSURANCE-_Guidance_Call_2019-5...</td>\n",
       "      <td>2019-05-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>423</td>\n",
       "      <td>Ladies and gentlemen good morning or good afte...</td>\n",
       "      <td>20160429_Swiss_Re_AG-_Earnings_Call_2016-4-29_...</td>\n",
       "      <td>2016-04-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>96</td>\n",
       "      <td>Welcome everyone to our presentation today Tha...</td>\n",
       "      <td>20200526_Aviva_PLC-_Shareholder_Mtg_Call_2020-...</td>\n",
       "      <td>2020-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>40</td>\n",
       "      <td>Operator Operator Instructions Good day ladies...</td>\n",
       "      <td>20191204_Sanlam_Ltd-_Sales_Results_Call_2019-1...</td>\n",
       "      <td>2019-12-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916</th>\n",
       "      <td>605</td>\n",
       "      <td>Greetings and welcome to the Lancashire Holdin...</td>\n",
       "      <td>20150729_LANCASHIRE_HOLDI-_Earnings_Call_2015-...</td>\n",
       "      <td>2015-07-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917</th>\n",
       "      <td>1115</td>\n",
       "      <td>Operator Good day and welcome to the Gjensidig...</td>\n",
       "      <td>20181025_GJENSIDIGE_FORSI-_Earnings_Call_2018-...</td>\n",
       "      <td>2018-10-25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>918 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                       meeting_text  \\\n",
       "0           749  Ladies and gentlemen Good afternoon Welcome de...   \n",
       "1          1492  Greetings Welcome to the Markel Corporation Fi...   \n",
       "2           446  Good morning or good afternoon Welcome to Swis...   \n",
       "3          1438  Market Cap 6903517 Current PX 14833 YTD Change...   \n",
       "4           337  Yasuyoshi Karasawa Good afternoon ladies and g...   \n",
       "..          ...                                                ...   \n",
       "913         423  Ladies and gentlemen good morning or good afte...   \n",
       "914          96  Welcome everyone to our presentation today Tha...   \n",
       "915          40  Operator Operator Instructions Good day ladies...   \n",
       "916         605  Greetings and welcome to the Lancashire Holdin...   \n",
       "917        1115  Operator Good day and welcome to the Gjensidig...   \n",
       "\n",
       "                                             file_name        date  \n",
       "0    20190424_AXA_SA-_Shareholder_Mtg_Call_2019-4-2...  2019-04-24  \n",
       "1    20120510_Markel_Corp-_Earnings_Call_2012-5-10_...  2012-05-10  \n",
       "2    20190731_Swiss_Re_AG-_Earnings_Call_2019-7-31_...  2019-07-31  \n",
       "3    20170726_Chubb_Ltd-_Earnings_Call_2017-7-26_FS...  2017-07-26  \n",
       "4    20190524_MS-AD_INSURANCE-_Guidance_Call_2019-5...  2019-05-24  \n",
       "..                                                 ...         ...  \n",
       "913  20160429_Swiss_Re_AG-_Earnings_Call_2016-4-29_...  2016-04-29  \n",
       "914  20200526_Aviva_PLC-_Shareholder_Mtg_Call_2020-...  2020-05-26  \n",
       "915  20191204_Sanlam_Ltd-_Sales_Results_Call_2019-1...  2019-12-04  \n",
       "916  20150729_LANCASHIRE_HOLDI-_Earnings_Call_2015-...  2015-07-29  \n",
       "917  20181025_GJENSIDIGE_FORSI-_Earnings_Call_2018-...  2018-10-25  \n",
       "\n",
       "[918 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.to_csv('df_sample_preprocess.csv',index=False)\n",
    "df = pd.read_csv('df_sample_preprocessed.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[E053] Could not read config.cfg from /Users/user/opt/anaconda3/lib/python3.8/site-packages/en_core_web_sm/en_core_web_sm-2.3.1/config.cfg",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-aabcc57a7cb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPhrases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0men_core_web_sm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0men_core_web_sm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m \u001b[0;31m# for spelling chcker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/en_core_web_sm/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(**overrides)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_init_py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model_from_init_py\u001b[0;34m(init_file, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE052\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m     return load_model_from_path(\n\u001b[0m\u001b[1;32m    518\u001b[0m         \u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model_from_path\u001b[0;34m(model_path, meta, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[0mconfig_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"config.cfg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[0moverrides\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_to_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m     \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model_from_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_config\u001b[0;34m(path, overrides, interpolate)\u001b[0m\n\u001b[1;32m    546\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconfig_path\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconfig_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconfig_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE053\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"config.cfg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m         return config.from_disk(\n\u001b[1;32m    550\u001b[0m             \u001b[0mconfig_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E053] Could not read config.cfg from /Users/user/opt/anaconda3/lib/python3.8/site-packages/en_core_web_sm/en_core_web_sm-2.3.1/config.cfg"
     ]
    }
   ],
   "source": [
    "# import the package\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import re\n",
    "import seaborn as sns\n",
    "# import packages\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# from spellchecker import SpellChecker\n",
    "from collections import Counter, OrderedDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import en_core_web_sm\n",
    "import spacy \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pyLDAvis\n",
    "from tracemalloc import stop\n",
    "from langcodes import best_match\n",
    "from matplotlib import rc\n",
    "import tomotopy as tp\n",
    "from gensim.models import Phrases\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "import pickle # for spelling chcker\n",
    "\n",
    "from rich.console import Console\n",
    "from rich.table import Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "reviews = df['meeting_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# define a data cleaning function\n",
    "def clean_data(reviews):\n",
    "    # # step 1: separate x and y\n",
    "    # convert review_text df to list\n",
    "    reviews = reviews.to_list()\n",
    "    \n",
    "    # step 2: nromalize the review\n",
    "    print(f\"Total voc number: {len(reviews)}\")\n",
    "    reviews = list(map(lambda x: x.lower(),reviews))\n",
    "\n",
    "    # step 3: remove Unicode Characters (punctuation, emoji,rul and @) \n",
    "    reviews = list(map(lambda x: re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", x),reviews))\n",
    "\n",
    "    # step 4: remove whitespaces and trailing spaces\n",
    "    def remove_whitespace(text):\n",
    "        pattern = re.compile(r'\\s+') \n",
    "        Without_whitespace = re.sub(pattern, ' ', text)\n",
    "        text = Without_whitespace.replace('?', ' ? ').replace(')', ') ')\n",
    "        text = text.strip()\n",
    "        return text\n",
    "    reviews=list(map(lambda x: remove_whitespace(x),reviews))\n",
    "                                                                                                                                                                                                                                                                                                    \n",
    "    # step 5: apply nlp pipeline remove stop words, numbers and lematize the words\n",
    "    \n",
    "    # expand the list of stopwords\n",
    "    gist_file = open(\"/Users/user/Documents/GitHub/data_collecting/testing/dataset/gist_stopwords.txt\", \"r\")\n",
    "    try:\n",
    "        content = gist_file.read()\n",
    "        stopwords = content.split(\",\")\n",
    "    finally:\n",
    "        gist_file.close()\n",
    "      \n",
    "    stopwords=[i.replace('\"',\"\").strip() for i in stopwords]\n",
    "    # add the stopwords to the list of stopwords\n",
    "    for i in stopwords:\n",
    "      nlp.Defaults.stop_words.add(i)\n",
    "\n",
    "    review  = pd.DataFrame({'text':reviews})\n",
    "\n",
    "    def tokenized_reviews(text):\n",
    "        return [str(token) for token in nlp(text) \n",
    "            if not token.is_stop \n",
    "            and not token.like_num\n",
    "            and not token.is_punct\n",
    "            and token.is_alpha\n",
    "            ]\n",
    "\n",
    "    def tag_reviews(text):\n",
    "        tags = ['NOUN','ADJ','VERB','ADV']\n",
    "        return [token.pos_ for token in nlp(text) \n",
    "                if not token.is_stop \n",
    "                and not token.like_num\n",
    "                and not token.is_punct\n",
    "                and token.is_alpha\n",
    "                # and token.pos_ in tags \n",
    "                ]\n",
    "\n",
    "    review['tokens'] = review['text'].apply(tokenized_reviews)\n",
    "    review['tags'] = review['text'].apply(tag_reviews)\n",
    "\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total voc number: 918\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-77d50e74e14f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# separate x and y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclean_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-c1d7c8c933b2>\u001b[0m in \u001b[0;36mclean_data\u001b[0;34m(reviews)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# add the stopwords to the list of stopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m       \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mreview\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mreviews\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nlp' is not defined"
     ]
    }
   ],
   "source": [
    "# separate x and y\n",
    "clean_text = clean_data(reviews)\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the cleaned data\n",
    "clean_text.to_csv('./dataset/clean_text.csv', index=False)\n",
    "# load the data \n",
    "clean_text = pd.read_csv('./dataset/clean_text.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the token\n",
    "def remove_brackets(text):\n",
    "    text = text.replace('[','')\n",
    "    text = text.replace(']','')\n",
    "    text = text.replace(\"'\",\"\")\n",
    "    text = text.split(', ')\n",
    "    return text\n",
    "\n",
    "# use apply lambda to apply the function to df_train_clean['tokens']\n",
    "temp_token = clean_text['tokens'].apply(remove_brackets)\n",
    "docs_tokens = []\n",
    "for i in temp_token:\n",
    "    docs_tokens.append(i)\n",
    "# docs_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word frequency\n",
    "# explore word frequencies\n",
    "word_freq = Counter(x for xs in docs_tokens for x in set(xs)).most_common()\n",
    "word_fr = pd.DataFrame(word_freq, columns=['word', 'freq']).sort_values(by='freq', ascending=False)\n",
    "\n",
    "# visualize word frequencies\n",
    "# figure size\n",
    "fig = plt.figure(figsize=(25, 6))\n",
    "plt.bar(word_fr.word[0:30], word_fr.freq[0:30], color='green')\n",
    "plt.xlabel(\"Word\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Word Frequency\")\n",
    "plt.show()\n",
    "word_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove most frequent words which is known in this context\n",
    "word_list = word_fr.head(3)['word'].to_list()\n",
    "mdy_train, tmp=[],[]\n",
    "for review in docs_tokens:\n",
    "    for word in review:\n",
    "        if word not in word_list:\n",
    "            tmp.append(word)\n",
    "    mdy_train.append(tmp)\n",
    "    tmp=[]\n",
    "\n",
    "clean_text['tokens'] = mdy_train\n",
    "clean_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic modeling\n",
    "# # 使用“短語”方法，我們訓練一個捕獲二元組的模型（參見步驟 1）和一個捕獲三元組的模型（參見步驟 2）。\n",
    "# common_terms = [\n",
    "# u\"of\",u\"with\",u\"without\",u\"and\",u\"or\",u\"the\",\n",
    "# u\"a\",u\"not\",u\"be\",u\"to\",u\"this\",u\"who\",u\"in\",\n",
    "# ]\n",
    "\n",
    "bigram = Phrases(                              # step 1 \n",
    "    docs_tokens,\n",
    "    min_count=50,\n",
    "    threshold=5,\n",
    "    max_vocab_size=50000,\n",
    "    # common_terms=common_terms,\n",
    ")\n",
    "trigram = Phrases(                             # step 2\n",
    "    bigram[docs_tokens],\n",
    "    min_count=50,\n",
    "    threshold=5,\n",
    "    max_vocab_size=50000,\n",
    "    # common_terms=common_terms,\n",
    ")\n",
    "# anks to these two models, \n",
    "# we can process the tokenized documents and join the tokens associated with bi- or tri-grams.\n",
    "# 借助這兩個模型，我們可以處理標記化的文檔並加入與二元組或三元組相關的標記。\n",
    "docs_phrased = [trigram[bigram[line]] for line in docs_tokens]\n",
    "# docs_phrased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the random seed to make the results reproducible\n",
    "np.random.seed(0)\n",
    "\n",
    "# 我們相信這些文檔足夠乾淨，可以訓練主題建模。\n",
    "# 使用 Tomotopy，有必要將預處理的文檔包裝在語料庫類對像中，\n",
    "# 該對象必須被初始化（步驟 1）和填充（步驟 2）。\n",
    "corpus = tp.utils.Corpus()                   # step 1 \n",
    "for item in docs_phrased:                    # step 2 這邊輸入我們的文檔\n",
    "    corpus.add_doc(words=item)\n",
    "\n",
    "# 題建模過程的一個組成部分是探索替代模型的擬合，即保留不同數量主題的模型。\n",
    "# 在此示例中，模型根據“u_ma​​ss”版本根據 Coherence Score 指標進行評估（分數越低，模型擬合越好）。\n",
    "# 在第 1 步中，我們創建一個空字典來存儲不同模型的分數。\n",
    "# 在第 2 步中，我們讓保留主題的數量在模型之間發生變化，並且對於每個模型，我們記錄相關的 Coherence Score\n",
    "cvs = {}                                    # step 1\n",
    "for topic_number in range(1, 31, 1):        # step 2\n",
    "    mdl = tp.LDAModel(k=topic_number, corpus=corpus) # 這邊輸入我們的文檔\n",
    "    for i in range(0, 100, 10): # 100 次迭代\n",
    "        mdl.train(10)\n",
    "        # print(\"Iteration: {}\\tLog-likelihood: {}\".format(i, mdl.ll_per_word))\n",
    "    coh = tp.coherence.Coherence(mdl, coherence=\"u_mass\")\n",
    "    cvs[topic_number] = coh.get_score() # 分數越低，模型擬合越好\n",
    "\n",
    "# coherence of the results how to fix???????\n",
    "\n",
    "# 這邊把csv視覺話 找最低分的\n",
    "# The below chart displays the fit of the fifteen competing models, \n",
    "# retaining between one and fifteen topics.\n",
    "# 下面的圖表顯示了 15 個競爭模型的擬合，保留 1 到 15 個主題。\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(cvs.keys(), cvs.values(), \"o-\")\n",
    "ax.set_xlabel(\"Number of topics retained\")\n",
    "ax.set_ylabel(\"Coherence score\")\n",
    "ax.set_xticks(range(1, 31, 1))\n",
    "plt.show()\n",
    "\n",
    "# 圖表的可視檢查表示最佳模型是有 11 個主題的模型。\n",
    "# 然而，為了提供更好的選擇，應該考慮更多的模型（例如在 1 到 200 主題的模型）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hence, we train our best fit model.\n",
    "# 所以，我們訓練我們的最佳模型。\n",
    "best_mdl = tp.LDAModel(k=13, corpus=corpus)\n",
    "for i in range(0, 100, 10):\n",
    "    best_mdl.train(10)\n",
    "    # print(\"Iteration: {}\\tLog-likelihood: {}\".format(i, best_mdl.ll_per_word))\n",
    "# Using the ‘get_topic_words’ attribute, \n",
    "# we retrieve the topic-to-word probabilities, \n",
    "# a key outcome of topic modeling. The estimated probabilities are arranged into a Pandas DF.\n",
    "# 使用“get_topic_words”屬性，我們可以取得主題到詞彙概率的預測，這是主題建模的一個重要結果。\n",
    "wt = pd.DataFrame()\n",
    "for k in range(best_mdl.k):\n",
    "    words, probs = [], []\n",
    "    for word, prob in best_mdl.get_topic_words(k):\n",
    "        words.append(word)\n",
    "        probs.append(prob)\n",
    "    tmp = pd.DataFrame(\n",
    "        {\n",
    "            \"word\": words,\n",
    "            \"prob\": np.round(probs, 3),\n",
    "            \"k\": np.repeat(k, len(words)),\n",
    "            \"sort\": np.arange(0, len(words)),\n",
    "        }\n",
    "    )\n",
    "    wt = pd.concat([wt, tmp], ignore_index=False)\n",
    "    del tmp\n",
    "wt #.head(1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The second outcome of topic modeling we retrieve consists of the topic-to-document probabilities,\n",
    "#  which we retrieve using the ‘get_topic_dist’ attribute, \n",
    "# available for any document included in the LDA corpus used for the training.\n",
    "# 第二個主題建模的結果是主題到文件概率的預測，\n",
    "# 這可以使用“get_topic_dist”屬性，可用於訓練 LDA 的資料庫中包含的任何文件。\n",
    "td = pd.DataFrame(\n",
    "    np.stack([doc.get_topic_dist() for doc in best_mdl.docs]),\n",
    "    columns=[\"topic_{}\".format(i + 1) for i in range(best_mdl.k)],\n",
    ")\n",
    "td #.head(1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_split_df['tokens'] = clean_text['tokens']\n",
    "paragraph_split_df['tags'] = clean_text['tags']\n",
    "paragraph_split_df = pd.concat([paragraph_split_df, td], axis=1)\n",
    "paragraph_split_df \n",
    "# internal and external participants put it in, add the frequency (single meeting, by the internal and external participants splitting )\n",
    "# sentiment_score weight by the length of sentnece"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "701d4f28dcd0866722f5108a3a4cbcd08882d26398b6d44117e791d7b8102912"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
